{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JafW-3RtEzj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppEpKVcK3Yfv",
        "outputId": "c2f194fc-2719-46dd-e2fe-c0b900c7a451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzEpG94N4Sdq",
        "outputId": "e0b190ed-2970-4b37-c815-dd777685a27b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: -r not specified; omitting directory '/content/drive/MyDrive/FlowNet'\n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/FlowNet Models-20230328T103823Z-001/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4jf4Cx7hvAB"
      },
      "outputs": [],
      "source": [
        "## Imporing the pretrained model\n",
        "import os\n",
        "y = torch.load(\"/content/drive/MyDrive/FlowNet/FlowNet_Models/pytorch/flownets_from_caffe.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAlcmYQur1T-",
        "outputId": "ae7b07af-d164-4ca7-ec7a-75f0b2e84f4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-78bd406b42a5>:10: ImportWarning: failed to load custom correlation modulewhich is needed for FlowNetC\n",
            "  warnings.warn(\"failed to load custom correlation module\"\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from spatial_correlation_sampler import spatial_correlation_sample\n",
        "except ImportError as e:\n",
        "    import warnings\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"default\", category=ImportWarning)\n",
        "        warnings.warn(\"failed to load custom correlation module\"\n",
        "                      \"which is needed for FlowNetC\", ImportWarning)\n",
        "\n",
        "\n",
        "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\n",
        "    if batchNorm:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
        "            nn.BatchNorm2d(out_planes),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )\n",
        "    else:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "def predict_flow(in_planes):\n",
        "    return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "\n",
        "\n",
        "def deconv(in_planes, out_planes):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.1,inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def correlate(input1, input2):\n",
        "    out_corr = spatial_correlation_sample(input1,\n",
        "                                          input2,\n",
        "                                          kernel_size=1,\n",
        "                                          patch_size=21,\n",
        "                                          stride=1,\n",
        "                                          padding=0,\n",
        "                                          dilation_patch=2)\n",
        "    # collate dimensions 1 and 2 in order to be treated as a\n",
        "    # regular 4D tensor\n",
        "    b, ph, pw, h, w = out_corr.size()\n",
        "    out_corr = out_corr.view(b, ph * pw, h, w)/input1.size(1)\n",
        "    return F.leaky_relu_(out_corr, 0.1)\n",
        "\n",
        "\n",
        "def crop_like(input, target):\n",
        "    if input.size()[2:] == target.size()[2:]:\n",
        "        return input\n",
        "    else:\n",
        "        return input[:, :, :target.size(2), :target.size(3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptwCwlzDkGA6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import kaiming_normal_, constant_\n",
        "# from .util import conv, predict_flow, deconv, crop_like\n",
        "\n",
        "__all__ = [\n",
        "    'flownets', 'flownets_bn'\n",
        "]\n",
        "\n",
        "\n",
        "class FlowNetS(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,batchNorm=True):\n",
        "        super(FlowNetS,self).__init__()\n",
        "\n",
        "        self.batchNorm = batchNorm\n",
        "        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2)\n",
        "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n",
        "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n",
        "        self.conv3_1 = conv(self.batchNorm, 256,  256)\n",
        "        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n",
        "        self.conv4_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n",
        "        self.conv5_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n",
        "        self.conv6_1 = conv(self.batchNorm,1024, 1024)\n",
        "\n",
        "        self.deconv5 = deconv(1024,512)\n",
        "        self.deconv4 = deconv(1026,256)\n",
        "        self.deconv3 = deconv(770,128)\n",
        "        self.deconv2 = deconv(386,64)\n",
        "\n",
        "        self.predict_flow6 = predict_flow(1024)\n",
        "        self.predict_flow5 = predict_flow(1026)\n",
        "        self.predict_flow4 = predict_flow(770)\n",
        "        self.predict_flow3 = predict_flow(386)\n",
        "        self.predict_flow2 = predict_flow(194)\n",
        "\n",
        "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                kaiming_normal_(m.weight, 0.1)\n",
        "                if m.bias is not None:\n",
        "                    constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                constant_(m.weight, 1)\n",
        "                constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_conv2 = self.conv2(self.conv1(x))\n",
        "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
        "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
        "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
        "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
        "\n",
        "        flow6       = self.predict_flow6(out_conv6)\n",
        "        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n",
        "        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n",
        "\n",
        "        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n",
        "        flow5       = self.predict_flow5(concat5)\n",
        "        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n",
        "        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n",
        "\n",
        "        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n",
        "        flow4       = self.predict_flow4(concat4)\n",
        "        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n",
        "        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n",
        "\n",
        "        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n",
        "        flow3       = self.predict_flow3(concat3)\n",
        "        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)\n",
        "        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)\n",
        "\n",
        "        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\n",
        "        flow2 = self.predict_flow2(concat2)\n",
        "\n",
        "        if self.training:\n",
        "            return flow2,flow3,flow4,flow5,flow6\n",
        "        else:\n",
        "            return flow2\n",
        "\n",
        "    def weight_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
        "\n",
        "    def bias_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'bias' in name]\n",
        "\n",
        "\n",
        "def flownets(data=None):\n",
        "    \"\"\"FlowNetS model architecture from the\n",
        "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
        "\n",
        "    Args:\n",
        "        data : pretrained weights of the network. will create a new one if not set\n",
        "    \"\"\"\n",
        "    model = FlowNetS(batchNorm=False)\n",
        "    if data is not None:\n",
        "        model.load_state_dict(data['state_dict'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def flownets_bn(data=None):\n",
        "    \"\"\"FlowNetS model architecture from the\n",
        "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
        "\n",
        "    Args:\n",
        "        data : pretrained weights of the network. will create a new one if not set\n",
        "    \"\"\"\n",
        "    model = FlowNetS(batchNorm=True)\n",
        "    if data is not None:\n",
        "        model.load_state_dict(data['state_dict'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1yTj_c8tsDy"
      },
      "outputs": [],
      "source": [
        "model = flownets(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYzynRIIvjIR"
      },
      "outputs": [],
      "source": [
        "# img1 = torch.randn((1,6,384,512))\n",
        "img1 = cv.imread(\"img1.jpg\")\n",
        "# img1 = np.float(img1)\n",
        "img2 = cv.imread(\"img2.jpg\")\n",
        "# img2 = float(img2)\n",
        "img = np.concatenate((img1,img2),axis = 2)\n",
        "img = np.swapaxes(img,0,2)\n",
        "img = np.swapaxes(img,1,2)\n",
        "img = img.reshape(1,6,384,512)\n",
        "img = np.array(img).astype('float32')\n",
        "img = torch.from_numpy(img)\n",
        "# model(img)\n",
        "# img1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap = cv.VideoCapture(\"Horses_2.mp4\")\n",
        "# ret, frame = vd.read()\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if (ret == True):\n",
        "        plt.imshow(frame)\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
